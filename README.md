[TOC]

# 20191027 讨论

## NMF

使用时间窗，估计脑功能网络序列。**时间窗口选取**，对准峰值。

- 比较正常人与病患脑网络区别

- 预测正常人的智力

### 区分性NMF

1. 综合Basis H
2. 正类 Basis H_1
3. 负类 Basis H_2

将三个Basis拼接，那么正类与负类在不同的Basis上得到的聚类矩阵不同。应用在脑功能数据上，具有可解释性。Discriminative NMF。

![1572452433668](C:\Users\hasee007\AppData\Roaming\Typora\typora-user-images\1572452433668.png)

### Nonlinear NMF

![1572154984294](C:\Users\hasee007\AppData\Roaming\Typora\typora-user-images\1572154984294.png)

## K-means

实现4个版本的K-means

1. K-means version 1 优化聚类中心和聚类矩阵。
2. K-means version 2 优化聚类中心，降低优化参数数量。
3. K-means version 3 优化聚类矩阵，行softmax与列归一化并行。softmax时的参数$\gamma$控制聚类矩阵的稀疏性，逐渐增大$\gamma$以逐渐增加聚类矩阵的稀疏性。
4. K-means version 4 优化聚类矩阵，行softmax与列归一化串行。

在高斯分布上验证K-means。

![1572452456927](C:\Users\hasee007\AppData\Roaming\Typora\typora-user-images\1572452456927.png)

## Deep Embedding

将高维特征的样本$X$映射到低维空间中，得到$Z = X(\theta)$。

# 实现思路

## Non-linear NMF

目标函数 $\min_{H, \theta} ||X - H \cdot <H, H>^{-1} <H, X>||_2^2$

- Encode：
  - 输入：样本矩阵$X_{d \times n}$ ；
  - 输出：特征矩阵$F_{k \times n}$；
  - 实现方式：n-layer MLP。 
- Decode：
  - 输入：特征矩阵$F_{k \times n}$；
  - 输出：解码矩阵$\widehat{X}_{d \times n}$；
  - 实现方式：n-layer MLP。
- Loss Function:
  - 目标函数 $\min_{H, \theta} ||X - \widehat{X}||_2^2$
- 如果Decode为1-layer全连接层，那么其等价于一个矩阵$\widehat{H}_{k \times d} \sim H$。

# 实验结果

## 分类结果
|     模型      |      数据集       |     acc     | epoch |
| :-----------: | :---------------: | :---------: | :---: |
|     SVM*      | Pos: 149 Neg: 103 | 0.70 ± 0.09 |   -   |
|     MLP*      | Pos: 149 Neg: 103 |   0.7692    |  900  |
| NMF_32-MLP**  | Pos: 149 Neg: 103 |   0.7692    |  500  |
| NMF_128-SVM** | Pos: 149 Neg: 103 | 0.68 ± 0.08 |   -   |
| NMF_128-MLP** | Pos: 149 Neg: 103 |   0.7308    |  800  |

*：以静态的皮尔森相关矩阵的上三角阵$Tri_p$为分类模型的输入，训练样本一共252个，特征维度为4371。

**：使用NMF降维后的稀疏矩阵作为分类模型的输入，NMF_[n_feature]。

## 分析讨论

- 线性NMF降低维度时损失大量信息，使降维后的特征矩阵在SVM和MLP分类器中的表现均出现下降。

- NMF降维可以加快MLP训练收敛速度。

- autoencoder获得编码其实较为稀疏

  ```python
  [   0.         0.         0.         0.         0.      1719.3164
      0.         0.         0.         0.         0.      4847.513
   1766.0596     0.      5593.446      0.         0.      4252.162
      0.         0.      2271.061      0.         0.      9220.86
      0.         0.         0.         0.         0.      3761.2168
      0.      3883.3926     0.         0.         0.         0.
      0.         0.         0.      5825.5146     0.         0.
      0.         0.      1779.1184     0.         0.         0.
      0.         0.         0.         0.         0.         0.
   1865.0522     0.      4841.545      0.      3798.3452     0.
      0.         0.         0.         0.         0.      8688.096
      0.         0.         0.        45.73517    0.         0.
    898.422      0.      2522.0115     0.         0.         0.
      0.         0.         0.         0.         0.         0.
      0.         0.      5247.8853     0.         0.         0.
   3359.8743     0.         0.         0.      3827.0417     0.
      0.         0.         0.      3222.352   3711.313      0.
      0.         0.         0.      1169.9457     0.      6952.536
      0.      4696.762      0.         0.         0.         0.
      0.         0.      1023.8358     0.      6213.2334  4552.6685
      0.      3149.803      0.      7628.554      0.         0.
   5526.59       0.     ]
  ```

- 